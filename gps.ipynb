{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94e353f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from control import BASE, policy\n",
    "import gym\n",
    "import torch\n",
    "import numpy as np\n",
    "import sys\n",
    "from torch import nn\n",
    "from NeuralNetwork import NN\n",
    "from utils import buffer\n",
    "import random\n",
    "import torch.onnx as onnx\n",
    "GAMMA = 0.98\n",
    "\n",
    "\n",
    "class GPS(BASE.BasePolicy):\n",
    "    def __init__(self, *args) -> None:\n",
    "        super().__init__(*args)\n",
    "        self.updatedPG = NN.ProbNN(self.o_s, self.h_s, self.a_index_s).to(self.device)\n",
    "        self.updatedDQN = NN.ValueNN(self.o_s, self.h_s, self.a_index_s).to(self.device)\n",
    "        self.baseDQN = NN.ValueNN(self.o_s, self.h_s, self.a_index_s).to(self.device)\n",
    "        self.baseDQN.eval()\n",
    "        self.policy = policy.Policy(self.cont, self.updatedPG, self.converter)\n",
    "        self.buffer = buffer.Simulate(self.env, self.policy, step_size=self.e_trace, done_penalty=self.d_p)\n",
    "        self.optimizer_p = torch.optim.SGD(self.updatedPG.parameters(), lr=self.lr)\n",
    "        self.optimizer_q = torch.optim.SGD(self.updatedDQN.parameters(), lr=self.lr)\n",
    "        self.criterion = nn.MSELoss(reduction='mean')\n",
    "\n",
    "    def get_policy(self):\n",
    "        return self.policy\n",
    "\n",
    "    def training(self, load=int(0)):\n",
    "\n",
    "        if int(load) == 1:\n",
    "            print(\"loading\")\n",
    "            self.updatedPG.load_state_dict(torch.load(self.PARAM_PATH + \"/1.pth\"))\n",
    "            self.updatedDQN.load_state_dict(torch.load(self.PARAM_PATH + \"/2.pth\"))\n",
    "            self.baseDQN.load_state_dict(self.updatedDQN.state_dict())\n",
    "            self.baseDQN.eval()\n",
    "            print(\"loading complete\")\n",
    "        else:\n",
    "            pass\n",
    "        i = 0\n",
    "        while i < self.t_i:\n",
    "            print(i)\n",
    "\n",
    "            i = i + 1\n",
    "            self.buffer.renewal_memory(self.ca, self.data, self.dataloader)\n",
    "            pg_loss, dqn_loss = self.train_per_buff()\n",
    "            self.writer.add_scalar(\"pg/loss\", pg_loss, i)\n",
    "            self.writer.add_scalar(\"dqn/loss\", dqn_loss, i)\n",
    "            self.writer.add_scalar(\"performance\", self.buffer.get_performance(), i)\n",
    "            torch.save(self.updatedPG.state_dict(), self.PARAM_PATH + \"/1.pth\")\n",
    "            torch.save(self.updatedDQN.state_dict(), self.PARAM_PATH + '/2.pth')\n",
    "            self.baseDQN.load_state_dict(self.updatedDQN.state_dict())\n",
    "            self.baseDQN.eval()\n",
    "\n",
    "        for param in self.updatedDQN.parameters():\n",
    "            print(\"----------dqn-------------\")\n",
    "            print(param)\n",
    "        for param in self.updatedPG.parameters():\n",
    "            print(\"----------pg--------------\")\n",
    "            print(param)\n",
    "\n",
    "        self.env.close()\n",
    "        self.writer.flush()\n",
    "        self.writer.close()\n",
    "\n",
    "    def train_per_buff(self):\n",
    "        i = 0\n",
    "        queue_loss = None\n",
    "        policy_loss = None\n",
    "        while i < self.m_i:\n",
    "            # print(i)\n",
    "            n_p_o, n_a, n_o, n_r, n_d = next(iter(self.dataloader))\n",
    "            t_p_o = torch.tensor(n_p_o, dtype=torch.float32).to(self.device)\n",
    "            t_a_index = self.converter.act2index(n_a, self.b_s).unsqueeze(axis=-1)\n",
    "            t_o = torch.tensor(n_o, dtype=torch.float32).to(self.device)\n",
    "            t_r = torch.tensor(n_r, dtype=torch.float32).to(self.device)\n",
    "            t_p_weight = torch.gather(self.updatedPG(t_p_o), 1, t_a_index)\n",
    "            t_p_qvalue = torch.gather(self.updatedDQN(t_p_o), 1, t_a_index)\n",
    "            weight = torch.transpose(torch.log(t_p_weight), 0, 1)\n",
    "            policy_loss = -torch.matmul(weight, t_p_qvalue)/self.b_s\n",
    "            t_trace = torch.tensor(n_d, dtype=torch.float32).to(self.device).unsqueeze(-1)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                n_a_expect = self.policy.select_action(n_o)\n",
    "                t_a_index = self.converter.act2index(n_a_expect, self.b_s).unsqueeze(-1)\n",
    "                t_qvalue = torch.gather(self.baseDQN(t_o), 1, t_a_index)\n",
    "                t_qvalue = t_qvalue*(GAMMA**t_trace) + t_r.unsqueeze(-1)\n",
    "\n",
    "            queue_loss = self.criterion(t_p_qvalue, t_qvalue)\n",
    "\n",
    "            self.optimizer_p.zero_grad()\n",
    "            policy_loss.backward(retain_graph=True)\n",
    "            for param in self.updatedPG.parameters():\n",
    "                param.grad.data.clamp_(-1, 1)\n",
    "            self.optimizer_p.step()\n",
    "\n",
    "            self.optimizer_q.zero_grad()\n",
    "            queue_loss.backward()\n",
    "            for param in self.updatedDQN.parameters():\n",
    "                param.grad.data.clamp_(-1, 1)\n",
    "            self.optimizer_q.step()\n",
    "\n",
    "            i = i + 1\n",
    "        print(\"loss1 = \", policy_loss)\n",
    "        print(\"loss2 = \", queue_loss)\n",
    "\n",
    "        return policy_loss, queue_loss\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
