{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f60dde0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.autograd.functional as F\n",
    "STATELEN = 10\n",
    "ACTLEN = 10\n",
    "STEP_SIZE = 4\n",
    "#based on https://homes.cs.washington.edu/~todorov/papers/TassaIROS12.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "12ec81d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dynamics(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(Dynamics, self).__init__()\n",
    "        # self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(input_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, output_size),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "    def forward(self, input_element):\n",
    "        output = self.linear_relu_stack(input_element)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "72e30322",
   "metadata": {},
   "outputs": [],
   "source": [
    "class reward(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size, hidden_size, output_size = 1):\n",
    "        super(reward, self).__init__()\n",
    "        # self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(input_size, hidden_size),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_size, output_size),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, input_element):\n",
    "        output = self.linear_relu_stack(input_element)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "112db9de",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_Dyna = Dynamics(STATELEN + ACTLEN, STATELEN, STATELEN)\n",
    "my_reward = reward(STATELEN + ACTLEN, STATELEN , 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95ce6946",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ilqr:\n",
    "    \n",
    "    def __init__(self, ts, dyn, re, sl, al):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            ts: time step\n",
    "            dyn: dynamic\n",
    "            re: reward\n",
    "            sl: state length\n",
    "            al: action length\n",
    "        \"\"\"\n",
    "        self.ts = ts\n",
    "        self.dyn = dyn\n",
    "        self.re = re\n",
    "        self.sl = sl\n",
    "        self.al = al\n",
    "        \n",
    "        self.S = torch.rand((self.ts, 1, self.sl))\n",
    "        self.A = torch.rand((self.ts, 1, self.al))\n",
    "        self.R = torch.empty((self.ts, 1, 1))\n",
    "        self.K_arr = torch.zeros(self.ts, self.al, self.sl)\n",
    "        self.k_arr = torch.zeros(self.ts, 1, self.al)\n",
    "        self.ifconv = 0\n",
    "\n",
    "    def _forward(self):\n",
    "        \n",
    "        p_S = self.S\n",
    "        p_A = self.A\n",
    "        s = p_S[0].clone().detach()\n",
    "        a = p_A[0].clone().detach()\n",
    "\n",
    "        i = 0\n",
    "        while i < self.time_step:\n",
    "            self.S[i] = s\n",
    "            a = (torch.matmul(s - p_S[i],torch.transpose((self.K_arr[i]),0,1)) + \n",
    "                 self.k_arr[i] + p_A[i]\n",
    "                )\n",
    "            self.A[i] = a\n",
    "            sa_in = torch.cat((s, self.A[i]),dim = 1)\n",
    "            #sa_in shape = [1,state_size + action_size]\n",
    "\n",
    "            s = self.dyn(sa_in)\n",
    "            #state shape = [1,state_size]\n",
    "\n",
    "            self.R[i] = self.re(sa_in)\n",
    "            i = i + 1\n",
    "\n",
    "    def _backward(self):\n",
    "        \n",
    "        i = self.ts -1\n",
    "        self.K_arr = torch.zeros(self.ts, self.al, self.sl )\n",
    "        self.k_arr = torch.zeros(self.ts, 1, self.al )\n",
    "\n",
    "        while i > -1:\n",
    "            sa_in = torch.cat((self.S[i], self.A[i]),dim = 1)\n",
    "            C_t = F.hessian(self.re, sa_in.view(-1))\n",
    "            #shape = [state+action, state+action]\n",
    "            #print(torch.sum(C_t))\n",
    "            F_t = F.jacobian(self.dyn, sa_in.view(-1))\n",
    "            transF_t = torch.transpose(F_t,0,1)\n",
    "            #shape = [state, state+action]\n",
    "            #print(torch.sum(F_t))\n",
    "            c_t = F.jacobian(self.re, sa_in.view(-1))\n",
    "            #shape = [1, state+action]\n",
    "            #print(torch.sum(c_t))\n",
    "\n",
    "            if i == self.ts - 1:\n",
    "                Q_t = C_t\n",
    "                q_t = c_t\n",
    "            else:\n",
    "                Q_t = C_t + torch.matmul(torch.matmul(transF_t, V_t), F_t)\n",
    "                #eq 5[c~e]\n",
    "                q_t = c_t + torch.matmul(v_t, F_t)\n",
    "                #eq 5[a~b]\n",
    "                \n",
    "            Q_pre1 = torch.split(Q_t, [self.sl, self.al])[0]\n",
    "            Q_pre2 = torch.split(Q_t, [self.sl, self.al])[1]\n",
    "            Q_xx = torch.split(Q_pre1, [self.sl, self.al], dim = 1)[0]\n",
    "            Q_xu = torch.split(Q_pre1, [self.sl, self.al], dim = 1)[1]\n",
    "            Q_ux = torch.split(Q_pre2, [self.sl, self.al], dim = 1)[0]\n",
    "            Q_uu = torch.split(Q_pre2, [self.sl, self.al], dim = 1)[1]\n",
    "            \n",
    "            q_t = torch.split(q_t, [self.sl, self.al], dim = 1)\n",
    "            Q_x = q_t[0]\n",
    "            Q_u = q_t[1]\n",
    "            \n",
    "            try:\n",
    "                invQuu = torch.linalg.inv(Q_uu - torch.eye(self.al)) #regularize term\n",
    "                #eq [9]\n",
    "            except:\n",
    "                invQuu = torch.linalg.inv(Q_uu + torch.eye(self.al)*0.01)\n",
    "                self.ifconv = 1\n",
    "\n",
    "            K_t = -torch.matmul(invQuu, Q_ux)\n",
    "            transK_t = torch.transpose(K_t, 0, 1)\n",
    "            #K_t shape = [actlen, statelen]\n",
    "            \n",
    "            k_t = -torch.matmul(Q_u, invQuu)\n",
    "            #k_t shape = [1,actlen]\n",
    "\n",
    "            V_t = (Q_xx + torch.matmul(Q_xu, K_t) + \n",
    "                   torch.matmul(transK_t, Q_ux) +\n",
    "                   torch.matmul(torch.matmul(transK_t, Q_uu), K_t)\n",
    "                  )\n",
    "            # eq 11c\n",
    "            #V_t shape = [statelen, statelen]\n",
    "\n",
    "            v_t = (Q_x + torch.matmul(k_t, Q_ux) + \n",
    "                   torch.matmul(Q_u, K_t) + \n",
    "                   torch.matmul(k_t, torch.matmul(Q_uu, K_t)) \n",
    "                  )\n",
    "            # eq 11b\n",
    "            #v_t shape = [1, statelen]\n",
    "            \n",
    "            self.K_arr[i] = K_t\n",
    "            self.k_arr[i] = k_t\n",
    "            i = i - 1\n",
    "    \n",
    "    def fit(self):\n",
    "        \n",
    "        i = 0\n",
    "        while(self.ifconv =! 1) and i < 100:\n",
    "            i = i + 1\n",
    "            self._forward()\n",
    "            self._backward()\n",
    "        \n",
    "        return self.A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "401e1700",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66677dc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for param in rew.parameters():\n",
    "#    print(param)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
