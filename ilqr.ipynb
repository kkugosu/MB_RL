{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f60dde0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.autograd.functional as F\n",
    "STATELEN = 10\n",
    "ACTLEN = 10\n",
    "STEP_SIZE = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "12ec81d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dynamics(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(Dynamics, self).__init__()\n",
    "        # self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(input_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, output_size),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "    def forward(self, input_element):\n",
    "        output = self.linear_relu_stack(input_element)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "72e30322",
   "metadata": {},
   "outputs": [],
   "source": [
    "class reward(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size, hidden_size, output_size = 1):\n",
    "        super(reward, self).__init__()\n",
    "        # self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(input_size, hidden_size),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_size, output_size),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, input_element):\n",
    "        output = self.linear_relu_stack(input_element)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "112db9de",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_Dyna = Dynamics(STATELEN + ACTLEN, STATELEN, STATELEN)\n",
    "my_reward = reward(STATELEN + ACTLEN, STATELEN , 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95ce6946",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ilqr:\n",
    "    \n",
    "    def __init__(self, ts, dyn, re, sl, al):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            ts: time step\n",
    "            dyn: dynamic\n",
    "            re: reward\n",
    "            sl: state length\n",
    "            al: action length\n",
    "        \"\"\"\n",
    "        self.ts = ts\n",
    "        self.dyn = dyn\n",
    "        self.re = re\n",
    "        self.sl = sl\n",
    "        self.al = al\n",
    "        \n",
    "        self.S = torch.rand((self.ts, 1, self.sl))\n",
    "        self.A = torch.rand((self.ts, 1, self.al))\n",
    "        self.R = torch.empty((self.ts, 1, 1))\n",
    "        self.K_arr = torch.zeros(self.ts, self.al, self.sl)\n",
    "        self.k_arr = torch.zeros(self.ts, 1, self.al)\n",
    "        self.ifconv = 0\n",
    "\n",
    "    def _forward(self):\n",
    "        \n",
    "        p_S = self.S\n",
    "        p_A = self.A\n",
    "        s = p_S[0].clone().detach()\n",
    "        a = p_A[0].clone().detach()\n",
    "\n",
    "        i = 0\n",
    "        while i < self.time_step:\n",
    "            self.S[i] = s\n",
    "            a = (torch.matmul(s - p_S[i],torch.transpose((self.K_arr[i]),0,1)) + \n",
    "                 self.k_arr[i] + p_A[i]\n",
    "                )\n",
    "            self.A[i] = a\n",
    "            sa_in = torch.cat((s, self.A[i]),dim = 1)\n",
    "            #sa_in shape = [1,state_size + action_size]\n",
    "\n",
    "            s = self.dyn(sa_in)\n",
    "            #state shape = [1,state_size]\n",
    "\n",
    "            self.R[i] = self.re(sa_in)\n",
    "            i = i + 1\n",
    "    \n",
    "\n",
    "    def _backward(self):\n",
    "        \n",
    "        i = self.time_step -1\n",
    "        self.K_arr = torch.zeros(self.ts, self.al ,self.sl )\n",
    "        self.k_arr = torch.zeros(self.ts, 1,self.al )\n",
    "\n",
    "        while i > -1:\n",
    "            sa_in = torch.cat((self.S[i],self.A[i]),dim = 1)\n",
    "            C_t = F.hessian(self.re, sa_in.view(-1))\n",
    "            #shape = [state+action, state+action]\n",
    "            #print(torch.sum(C_t))\n",
    "            F_t = F.jacobian(self.dyn, sa_in.view(-1))\n",
    "            #shape = [state, state+action]\n",
    "            #print(torch.sum(F_t))\n",
    "            c_t = F.jacobian(self.re, sa_in.view(-1))\n",
    "            #shape = [1, state+action]\n",
    "            #print(torch.sum(c_t))\n",
    "\n",
    "            if i == step_size-1:\n",
    "                Q_t = C_t\n",
    "                q_t = c_t\n",
    "            else:\n",
    "                Q_t = C_t + torch.matmul(torch.matmul(torch.transpose(F_t,0,1),V_t),F_t)\n",
    "                q_t = c_t + torch.matmul(v_t,F_t)\n",
    "\n",
    "            Q_pre1 = torch.split(Q_t,[self.sl, self.al])[0]\n",
    "            Q_pre2 = torch.split(Q_t,[self.sl, self.al])[1]\n",
    "            Q_xx = torch.split(Q_pre1,[self.sl, self.al],dim = 1)[0]\n",
    "            Q_xu = torch.split(Q_pre1,[self.sl, self.al],dim = 1)[1]\n",
    "            Q_uu = torch.split(Q_pre2,[self.sl, self.al],dim = 1)[1]\n",
    "\n",
    "            try:\n",
    "                invQuu = torch.linalg.inv(Q_uu - torch.eye(self.al)) #regularize term\n",
    "            except:\n",
    "                invQuu = torch.linalg.inv(Q_uu + torch.eye(self.al)*0.01)\n",
    "                self.ifconv = 1\n",
    "\n",
    "            K_t = -torch.matmul(invQuu, torch.transpose(Q_xu,0,1))\n",
    "            transK_t = torch.transpose(K_t,0,1)\n",
    "            #K_t shape = [actlen, statelen]\n",
    "            q_t = torch.split(q_t, [self.sl, self.al], dim = 1)\n",
    "            k_t = -torch.matmul(q_t[1],invQuu)\n",
    "            #k_t shape = [1,actlen]\n",
    "\n",
    "            V_t = (Q_xx + torch.matmul(Q_xu,K_t)*2 + \n",
    "                   torch.matmul(torch.matmul(transK_t,Q_uu),K_t)\n",
    "                  )\n",
    "            #V_t shape = [statelen, statelen]\n",
    "\n",
    "            v_t = (q_t[0] + torch.matmul(k_t,torch.transpose(Q_xu,0,1)) + \n",
    "                   torch.matmul(q_t[1],K_t) + \n",
    "                   torch.matmul(k_t, torch.matmul(Q_uu,K_t)) \n",
    "                  )\n",
    "            #v_t shape = [1, statelen]\n",
    "            self.K_arr[i] = K_t\n",
    "            self.k_arr[i] = k_t\n",
    "            i = i - 1\n",
    "    \n",
    "    \n",
    "    def fit(self):\n",
    "        \n",
    "        i = 0\n",
    "        while(self.ifconv =! 1) and i < 100:\n",
    "            i = i + 1\n",
    "            self._forward()\n",
    "            self._backward()\n",
    "        \n",
    "        return self.A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2b0006a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1bcf7242",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "tensor([[0.8255, 0.8293, 0.3522, 0.4119, 0.4937, 0.9669, 0.4103, 0.9699, 0.7606,\n",
      "         0.0503]], grad_fn=<SelectBackward0>)\n",
      "100\n",
      "tensor([[-2.0106, -2.7740,  0.6842,  2.0354,  2.4880, -0.1632,  2.4594, -1.8509,\n",
      "          0.1161, -3.2119]], grad_fn=<SelectBackward0>)\n",
      "200\n",
      "tensor([[-2.6977, -3.2965,  0.3127,  2.6208,  3.1344, -0.9797,  3.6866, -2.7824,\n",
      "          0.9371, -4.5258]], grad_fn=<SelectBackward0>)\n",
      "300\n",
      "tensor([[-3.1273, -3.5203,  0.0622,  2.9640,  3.4330, -1.5143,  4.4964, -3.3415,\n",
      "          1.5872, -5.4317]], grad_fn=<SelectBackward0>)\n",
      "400\n",
      "tensor([[-3.4153, -3.6490, -0.0937,  3.1752,  3.6010, -1.8778,  5.1152, -3.7635,\n",
      "          2.0480, -6.1381]], grad_fn=<SelectBackward0>)\n",
      "500\n",
      "tensor([[-3.6205, -3.7296, -0.1944,  3.3124,  3.7031, -2.1374,  5.6245, -4.0997,\n",
      "          2.3910, -6.7335]], grad_fn=<SelectBackward0>)\n",
      "600\n",
      "tensor([[-3.7764, -3.7823, -0.2620,  3.4051,  3.7674, -2.3316,  6.0594, -4.3761,\n",
      "          2.6594, -7.2538]], grad_fn=<SelectBackward0>)\n",
      "700\n",
      "tensor([[-3.8994, -3.8194, -0.3132,  3.4762,  3.8159, -2.4828,  6.4348, -4.6075,\n",
      "          2.8800, -7.7144]], grad_fn=<SelectBackward0>)\n",
      "800\n",
      "tensor([[-4.0028, -3.8498, -0.3558,  3.5393,  3.8599, -2.6071,  6.7604, -4.8051,\n",
      "          3.0694, -8.1206]], grad_fn=<SelectBackward0>)\n",
      "900\n",
      "tensor([[-4.0977, -3.8844, -0.3915,  3.5988,  3.9088, -2.7158,  7.0459, -4.9763,\n",
      "          3.2321, -8.4738]], grad_fn=<SelectBackward0>)\n",
      "1000\n",
      "tensor([[-4.1875, -3.9225, -0.4238,  3.6590,  3.9634, -2.8142,  7.2969, -5.1262,\n",
      "          3.3774, -8.7814]], grad_fn=<SelectBackward0>)\n",
      "1100\n",
      "tensor([[-4.2734, -3.9610, -0.4545,  3.7212,  4.0201, -2.9044,  7.5189, -5.2602,\n",
      "          3.5127, -9.0517]], grad_fn=<SelectBackward0>)\n",
      "1200\n",
      "tensor([[-4.3556, -3.9998, -0.4840,  3.7848,  4.0780, -2.9883,  7.7174, -5.3822,\n",
      "          3.6392, -9.2909]], grad_fn=<SelectBackward0>)\n",
      "1300\n",
      "tensor([[-4.4339, -4.0384, -0.5123,  3.8485,  4.1358, -3.0667,  7.8967, -5.4947,\n",
      "          3.7577, -9.5046]], grad_fn=<SelectBackward0>)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [6]\u001b[0m, in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m i \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m10000\u001b[39m:\n\u001b[1;32m     11\u001b[0m     state_col, act_col ,reward_col \u001b[38;5;241m=\u001b[39m forward_step(state_col, act_col, K_arr, k_arr, STEP_SIZE)\n\u001b[0;32m---> 12\u001b[0m     K_arr, k_arr, ifconv \u001b[38;5;241m=\u001b[39m \u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate_col\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mact_col\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreward_col\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mSTEP_SIZE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mifconv\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i\u001b[38;5;241m%\u001b[39m\u001b[38;5;241m100\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     14\u001b[0m         \u001b[38;5;28mprint\u001b[39m(i)\n",
      "Input \u001b[0;32mIn [4]\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(S, A, R, step_size, ifconv)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m i \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m     35\u001b[0m     sa_in \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat((S[i],A[i]),dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 36\u001b[0m     C_t \u001b[38;5;241m=\u001b[39m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhessian\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmy_reward\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msa_in\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;66;03m#shape = [state+action, state+action]\u001b[39;00m\n\u001b[1;32m     38\u001b[0m     \u001b[38;5;66;03m#print(torch.sum(C_t))\u001b[39;00m\n\u001b[1;32m     39\u001b[0m     F_t \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mjacobian(my_Dyna, sa_in\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/autograd/functional.py:807\u001b[0m, in \u001b[0;36mhessian\u001b[0;34m(func, inputs, create_graph, strict, vectorize, outer_jacobian_strategy)\u001b[0m\n\u001b[1;32m    804\u001b[0m     _check_requires_grad(jac, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjacobian\u001b[39m\u001b[38;5;124m\"\u001b[39m, strict\u001b[38;5;241m=\u001b[39mstrict)\n\u001b[1;32m    805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m jac\n\u001b[0;32m--> 807\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43mjacobian\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjac_func\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstrict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstrict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvectorize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvectorize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    808\u001b[0m \u001b[43m               \u001b[49m\u001b[43mstrategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mouter_jacobian_strategy\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    809\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _tuple_postprocess(res, (is_inputs_tuple, is_inputs_tuple))\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/autograd/functional.py:669\u001b[0m, in \u001b[0;36mjacobian\u001b[0;34m(func, inputs, create_graph, strict, vectorize, strategy)\u001b[0m\n\u001b[1;32m    667\u001b[0m jac_i: Tuple[List[torch\u001b[38;5;241m.\u001b[39mTensor]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m([] \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(inputs)))  \u001b[38;5;66;03m# type: ignore[assignment]\u001b[39;00m\n\u001b[1;32m    668\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(out\u001b[38;5;241m.\u001b[39mnelement()):\n\u001b[0;32m--> 669\u001b[0m     vj \u001b[38;5;241m=\u001b[39m _autograd_grad((\u001b[43mout\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m[j],), inputs,\n\u001b[1;32m    670\u001b[0m                         retain_graph\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, create_graph\u001b[38;5;241m=\u001b[39mcreate_graph)\n\u001b[1;32m    672\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m el_idx, (jac_i_el, vj_el, inp_el) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mzip\u001b[39m(jac_i, vj, inputs)):\n\u001b[1;32m    673\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m vj_el \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "i = 0\n",
    "while i < 10000:\n",
    "    \n",
    "    state_col, act_col ,reward_col = forward_step(state_col, act_col, K_arr, k_arr, STEP_SIZE)\n",
    "    K_arr, k_arr, ifconv = backward(state_col, act_col, reward_col, STEP_SIZE, ifconv)\n",
    "    if i%100 == 0:\n",
    "        print(i)\n",
    "        print(act_col[0])\n",
    "    if ifconv == 1:\n",
    "        break\n",
    "    \n",
    "    i = i + 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66677dc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for param in rew.parameters():\n",
    "#    print(param)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
