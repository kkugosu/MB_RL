{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f160c3b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.autograd.functional as F\n",
    "STATELEN = 10\n",
    "ACTLEN = 10\n",
    "STEP_SIZE = 4\n",
    "#based on https://homes.cs.washington.edu/~todorov/papers/TassaIROS12.pdf\n",
    "class Dynamics(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(Dynamics, self).__init__()\n",
    "        # self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(input_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, output_size),\n",
    "            nn.LeakyReLU(0.1)\n",
    "        )\n",
    "    def forward(self, input_element):\n",
    "        output = self.linear_relu_stack(input_element)\n",
    "        return output\n",
    "    \n",
    "class reward(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size, hidden_size, output_size = 1):\n",
    "        super(reward, self).__init__()\n",
    "        # self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(input_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, output_size),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, input_element):\n",
    "        output = self.linear_relu_stack(input_element)\n",
    "        return output\n",
    "    \n",
    "class ilqr:\n",
    "    \n",
    "    def __init__(self, ts, dyn, re, sl, al):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            ts: time step\n",
    "            dyn: dynamic\n",
    "            re: reward\n",
    "            sl: state length\n",
    "            al: action length\n",
    "        \"\"\"\n",
    "        self.ts = ts\n",
    "        self.dyn = dyn\n",
    "        self.re = re\n",
    "        self.sl = sl\n",
    "        self.al = al\n",
    "        self.b_s = 1\n",
    "        \n",
    "        self.S = torch.rand((self.ts, self.b_s, self.sl))\n",
    "        self.A = torch.rand((self.ts, self.b_s, self.al))\n",
    "        self.R = torch.empty((self.ts, self.b_s, 1))\n",
    "        self.K_arr = torch.zeros(self.ts, self.b_s, self.al, self.sl)\n",
    "        self.k_arr = torch.zeros(self.ts, self.b_s, 1, self.al)\n",
    "        self.V_t = torch.zeros(self.b_s, self.sl, self.sl)\n",
    "        self.v_t = torch.zeros(self.b_s, 1, self.sl)\n",
    "        self.ifconv = 0\n",
    "        \n",
    "    def _forward(self):\n",
    "        \n",
    "        new_S = torch.rand((self.ts, self.b_s, self.sl))\n",
    "        new_A = torch.rand((self.ts, self.b_s, self.al))\n",
    "        s = self.S[0].clone().detach()\n",
    "\n",
    "        i = 0\n",
    "        while i < self.ts:\n",
    "            new_S[i] = s\n",
    "            new_A[i] = (torch.matmul(new_S[i] - self.S[i],torch.transpose((self.K_arr[i]),0,1)) + \n",
    "                        self.k_arr[i] + self.A[i]\n",
    "                       )\n",
    "            \n",
    "            sa_in = torch.cat((new_S[i], new_A[i]),dim = 1)\n",
    "            #sa_in shape = [1,state_size + action_size]\n",
    "\n",
    "            s = self.dyn(sa_in)\n",
    "            #state shape = [1,state_size]\n",
    "\n",
    "            self.R[i] = self.re(sa_in)\n",
    "            \n",
    "            i = i + 1\n",
    "            \n",
    "        self.S = new_S\n",
    "        self.A = new_A\n",
    "\n",
    "    def _backward(self):\n",
    "        \n",
    "        C = torch.zeros(self.b_s, self.al + self.sl, self.al + self.sl)\n",
    "        F = torch.zeros(self.b_s, self.sl, self.al + self.sl)\n",
    "        c = torch.zeros(self.b_s, 1, self.al + self.sl)\n",
    "        sa_in = torch.cat((self.S, self.A),dim = 2)\n",
    "        i = self.ts -1\n",
    "        while i > -1:\n",
    "            j = 0\n",
    "            while j < self.b_s:\n",
    "                C[j] = F.hessian(self.re, sa_in[i][j])\n",
    "                #shape = [state+action, state+action]\n",
    "                #print(torch.sum(C[j]))\n",
    "                F[j] = F.jacobian(self.dyn, sa_in[i][j])\n",
    "                #shape = [state, state+action]\n",
    "                #print(torch.sum(F[j]))\n",
    "                c[j] = F.jacobian(self.re, sa_in[i][j])\n",
    "                #shape = [1, state+action]\n",
    "                #print(torch.sum(c[j]))\n",
    "                j = j + 1\n",
    "            \n",
    "            transF_t = torch.transpose(F_t,1,2)\n",
    "            Q = C + torch.matmul(torch.matmul(transF_t, V_t), F_t)\n",
    "            #eq 5[c~e]\n",
    "            q = c + torch.matmul(v_t, F_t)\n",
    "            #eq 5[a~b]\n",
    "            \n",
    "            Q_pre1, Q_pre2 = torch.split(Q, [self.sl, self.al], dim = 1)\n",
    "            Q_xx, Q_xu = torch.split(Q_pre1, [self.sl, self.al], dim = 2)\n",
    "            Q_ux, Q_uu = torch.split(Q_pre2, [self.sl, self.al], dim = 2)\n",
    "            \n",
    "            Q_x, Q_u = torch.split(q, [self.sl, self.al], dim = 1)\n",
    "            ## how to batched eye?\n",
    "            try:\n",
    "                invQuu = torch.linalg.inv(Q_uu - torch.eye(self.al)) #regularize term\n",
    "                #eq [9]\n",
    "            except:\n",
    "                invQuu = torch.linalg.inv(Q_uu + torch.eye(self.al)*0.01)\n",
    "                self.ifconv = 1\n",
    "\n",
    "            K_t = -torch.matmul(invQuu, Q_ux)\n",
    "            transK_t = torch.transpose(K_t, 0, 1)\n",
    "            #K_t shape = [actlen, statelen]\n",
    "            \n",
    "            k_t = -torch.matmul(Q_u, invQuu)\n",
    "            #k_t shape = [1,actlen]\n",
    "            \n",
    "            V_t = (Q_xx + torch.matmul(Q_xu, K_t) + \n",
    "                   torch.matmul(transK_t, Q_ux) +\n",
    "                   torch.matmul(torch.matmul(transK_t, Q_uu), K_t)\n",
    "                  )\n",
    "            # eq 11c\n",
    "            #V_t shape = [statelen, statelen]\n",
    "\n",
    "            v_t = (Q_x + torch.matmul(k_t, Q_ux) + \n",
    "                   torch.matmul(Q_u, K_t) + \n",
    "                   torch.matmul(k_t, torch.matmul(Q_uu, K_t)) \n",
    "                  )\n",
    "            # eq 11b\n",
    "            #v_t shape = [1, statelen]\n",
    "            \n",
    "            self.K_arr[i] = K_t\n",
    "            self.k_arr[i] = k_t\n",
    "            i = i - 1\n",
    "    \n",
    "    def fit(self):\n",
    "        \n",
    "        i = 0\n",
    "        while (self.ifconv != 1) and i < 100:\n",
    "            i = i + 1\n",
    "            self._forward()\n",
    "            self._backward()\n",
    "        \n",
    "        return self.A\n",
    " \n",
    "#for param in rew.parameters():\n",
    "#    print(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "036a84a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-2.2658, -0.9684,  4.6181,  0.6727, -2.4023,  0.1251,  0.3206,\n",
       "           1.6152,  0.2583, -1.9794]],\n",
       "\n",
       "        [[-1.9366, -1.3025,  4.8335,  0.4203, -2.4425,  0.0961,  0.3086,\n",
       "           2.1336,  0.5074, -2.2155]],\n",
       "\n",
       "        [[-2.4339, -1.0471,  4.4494,  0.2337, -2.2584,  0.0066,  0.5277,\n",
       "           2.1778,  0.5146, -2.4784]],\n",
       "\n",
       "        [[-1.9440, -1.3290,  4.7818,  0.5538, -2.6536,  0.0823,  0.4539,\n",
       "           2.0534,  0.3851, -2.2354]]], grad_fn=<CopySlices>)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_Dyna = Dynamics(STATELEN + ACTLEN, STATELEN, STATELEN)\n",
    "my_reward = reward(STATELEN + ACTLEN, STATELEN , 1)\n",
    "\n",
    "\n",
    "myilqr = ilqr(4,my_Dyna,my_reward,10,10)\n",
    "\n",
    "myilqr.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f4062449",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "S = torch.rand((3, 1, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "32d483ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([12])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.shape(S.view(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "05f08e67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 4])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(S.squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dca2265f",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'self' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [11]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m new_S \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrand((\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241m.\u001b[39mts, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msl))\n\u001b[0;32m      2\u001b[0m new_A \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrand((\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mts, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mal))\n\u001b[0;32m      4\u001b[0m my_Dyna \u001b[38;5;241m=\u001b[39m Dynamics(STATELEN \u001b[38;5;241m+\u001b[39m ACTLEN, STATELEN, STATELEN)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'self' is not defined"
     ]
    }
   ],
   "source": [
    "new_S = torch.rand((4, 1, 3))\n",
    "new_A = torch.rand((4, 1, 5))\n",
    "\n",
    "my_Dyna = Dynamics(STATELEN + ACTLEN, STATELEN, STATELEN)\n",
    "my_reward = reward(STATELEN + ACTLEN, STATELEN , 1)\n",
    "\n",
    "s = S[0].clone().detach()\n",
    "\n",
    "\n",
    "i = 0\n",
    "while i < self.ts:\n",
    "    new_S[i] = s\n",
    "    new_A[i] = (torch.matmul(new_S[i] - S[i],torch.transpose((K_arr[i]),0,1)) + \n",
    "         k_arr[i] + A[i]\n",
    "        )\n",
    "    sa_in = torch.cat((new_S[i], new_A[i]),dim = 1)\n",
    "    #sa_in shape = [1,state_size + action_size]\n",
    "\n",
    "    s = my_Dyna(sa_in)\n",
    "    #state shape = [1,state_size]\n",
    "\n",
    "    R[i] = my_reward(sa_in)\n",
    "\n",
    "    i = i + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c7b8b74f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.2503, 0.3700, 0.4463],\n",
      "         [0.4160, 0.0699, 0.1047]],\n",
      "\n",
      "        [[0.9365, 0.2305, 0.5194],\n",
      "         [0.5007, 0.1840, 0.7685]],\n",
      "\n",
      "        [[0.3118, 0.0037, 0.9661],\n",
      "         [0.8170, 0.1456, 0.4000]],\n",
      "\n",
      "        [[0.2563, 0.4554, 0.3776],\n",
      "         [0.1245, 0.0504, 0.4079]]])\n",
      "tensor([[[3.6793e-02, 1.1044e-01, 2.9308e-04, 4.6370e-01, 4.4860e-01],\n",
      "         [3.6057e-01, 9.2463e-03, 5.8263e-01, 5.1973e-01, 1.9103e-01]],\n",
      "\n",
      "        [[2.8883e-01, 3.3326e-01, 8.0322e-01, 9.9161e-01, 5.6124e-01],\n",
      "         [2.3160e-01, 5.5209e-02, 6.0231e-02, 1.4384e-02, 5.6236e-01]],\n",
      "\n",
      "        [[4.1703e-01, 1.2557e-01, 1.8472e-01, 7.6195e-01, 3.2007e-02],\n",
      "         [2.8687e-01, 4.0119e-01, 6.5657e-02, 4.1373e-01, 3.4538e-01]],\n",
      "\n",
      "        [[8.0777e-01, 2.9345e-01, 5.2441e-01, 6.9381e-01, 3.9671e-01],\n",
      "         [9.2216e-01, 5.5927e-01, 8.9263e-01, 2.1028e-01, 6.0275e-01]]])\n",
      "torch.Size([2, 8])\n"
     ]
    }
   ],
   "source": [
    "new_S = torch.rand((4, 2, 3))\n",
    "new_A = torch.rand((4, 2, 5))\n",
    "print(new_S)\n",
    "print(new_A)\n",
    "sa_in = torch.cat((new_S[0], new_A[0]),dim = 1)\n",
    "print(np.shape(sa_in))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ba9b506a",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_Dyna = Dynamics(8, 3, 3)\n",
    "my_reward = reward(8, 3 , 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1e141ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = my_Dyna(torch.cat((new_S[0], new_A[0]),dim = 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "73d74918",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "55d74156",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The Tensor returned by the function given to hessian should contain a single element",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[1;32mIn [26]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mautograd\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctional\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mF\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m C_t \u001b[38;5;241m=\u001b[39m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhessian\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmy_reward\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msa_in\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(np\u001b[38;5;241m.\u001b[39mshape(C_t))\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\autograd\\functional.py:807\u001b[0m, in \u001b[0;36mhessian\u001b[1;34m(func, inputs, create_graph, strict, vectorize, outer_jacobian_strategy)\u001b[0m\n\u001b[0;32m    804\u001b[0m     _check_requires_grad(jac, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjacobian\u001b[39m\u001b[38;5;124m\"\u001b[39m, strict\u001b[38;5;241m=\u001b[39mstrict)\n\u001b[0;32m    805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m jac\n\u001b[1;32m--> 807\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43mjacobian\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjac_func\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstrict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstrict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvectorize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvectorize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    808\u001b[0m \u001b[43m               \u001b[49m\u001b[43mstrategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mouter_jacobian_strategy\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    809\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _tuple_postprocess(res, (is_inputs_tuple, is_inputs_tuple))\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\autograd\\functional.py:574\u001b[0m, in \u001b[0;36mjacobian\u001b[1;34m(func, inputs, create_graph, strict, vectorize, strategy)\u001b[0m\n\u001b[0;32m    571\u001b[0m is_inputs_tuple, inputs \u001b[38;5;241m=\u001b[39m _as_tuple(inputs, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minputs\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjacobian\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    572\u001b[0m inputs \u001b[38;5;241m=\u001b[39m _grad_preprocess(inputs, create_graph\u001b[38;5;241m=\u001b[39mcreate_graph, need_graph\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m--> 574\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    575\u001b[0m is_outputs_tuple, outputs \u001b[38;5;241m=\u001b[39m _as_tuple(outputs,\n\u001b[0;32m    576\u001b[0m                                       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutputs of the user-provided function\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    577\u001b[0m                                       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjacobian\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    578\u001b[0m _check_requires_grad(outputs, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutputs\u001b[39m\u001b[38;5;124m\"\u001b[39m, strict\u001b[38;5;241m=\u001b[39mstrict)\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\autograd\\functional.py:803\u001b[0m, in \u001b[0;36mhessian.<locals>.jac_func\u001b[1;34m(*inp)\u001b[0m\n\u001b[0;32m    799\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m outer_jacobian_strategy \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mforward-mode\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    800\u001b[0m     \u001b[38;5;66;03m# _grad_preprocess requires create_graph=True and input to require_grad\u001b[39;00m\n\u001b[0;32m    801\u001b[0m     \u001b[38;5;66;03m# or else the input will be detached\u001b[39;00m\n\u001b[0;32m    802\u001b[0m     inp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(t\u001b[38;5;241m.\u001b[39mrequires_grad_(\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m inp)\n\u001b[1;32m--> 803\u001b[0m jac \u001b[38;5;241m=\u001b[39m \u001b[43mjacobian\u001b[49m\u001b[43m(\u001b[49m\u001b[43mensure_single_output_function\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    804\u001b[0m _check_requires_grad(jac, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjacobian\u001b[39m\u001b[38;5;124m\"\u001b[39m, strict\u001b[38;5;241m=\u001b[39mstrict)\n\u001b[0;32m    805\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m jac\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\autograd\\functional.py:574\u001b[0m, in \u001b[0;36mjacobian\u001b[1;34m(func, inputs, create_graph, strict, vectorize, strategy)\u001b[0m\n\u001b[0;32m    571\u001b[0m is_inputs_tuple, inputs \u001b[38;5;241m=\u001b[39m _as_tuple(inputs, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minputs\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjacobian\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    572\u001b[0m inputs \u001b[38;5;241m=\u001b[39m _grad_preprocess(inputs, create_graph\u001b[38;5;241m=\u001b[39mcreate_graph, need_graph\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m--> 574\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    575\u001b[0m is_outputs_tuple, outputs \u001b[38;5;241m=\u001b[39m _as_tuple(outputs,\n\u001b[0;32m    576\u001b[0m                                       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutputs of the user-provided function\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    577\u001b[0m                                       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjacobian\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    578\u001b[0m _check_requires_grad(outputs, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutputs\u001b[39m\u001b[38;5;124m\"\u001b[39m, strict\u001b[38;5;241m=\u001b[39mstrict)\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\autograd\\functional.py:794\u001b[0m, in \u001b[0;36mhessian.<locals>.ensure_single_output_function\u001b[1;34m(*inp)\u001b[0m\n\u001b[0;32m    791\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe function given to hessian should return a single Tensor\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    793\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m out\u001b[38;5;241m.\u001b[39mnelement() \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m--> 794\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe Tensor returned by the function given to hessian should contain a single element\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    796\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out\u001b[38;5;241m.\u001b[39msqueeze()\n",
      "\u001b[1;31mRuntimeError\u001b[0m: The Tensor returned by the function given to hessian should contain a single element"
     ]
    }
   ],
   "source": [
    "import torch.autograd.functional as F\n",
    "C_t = F.hessian(my_reward, sa_in)\n",
    "print(np.shape(C_t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "115224b4",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "pow_adder_reducer() missing 1 required positional argument: 'y'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [30]\u001b[0m, in \u001b[0;36m<cell line: 5>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m    \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m x\u001b[38;5;241m.\u001b[39mpow(\u001b[38;5;241m2\u001b[39m) \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m3\u001b[39m \u001b[38;5;241m*\u001b[39m y\u001b[38;5;241m.\u001b[39mpow(\u001b[38;5;241m2\u001b[39m))\u001b[38;5;241m.\u001b[39msum()\n\u001b[0;32m      4\u001b[0m inputs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrand(\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m----> 5\u001b[0m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhessian\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpow_adder_reducer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\autograd\\functional.py:807\u001b[0m, in \u001b[0;36mhessian\u001b[1;34m(func, inputs, create_graph, strict, vectorize, outer_jacobian_strategy)\u001b[0m\n\u001b[0;32m    804\u001b[0m     _check_requires_grad(jac, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjacobian\u001b[39m\u001b[38;5;124m\"\u001b[39m, strict\u001b[38;5;241m=\u001b[39mstrict)\n\u001b[0;32m    805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m jac\n\u001b[1;32m--> 807\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43mjacobian\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjac_func\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstrict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstrict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvectorize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvectorize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    808\u001b[0m \u001b[43m               \u001b[49m\u001b[43mstrategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mouter_jacobian_strategy\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    809\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _tuple_postprocess(res, (is_inputs_tuple, is_inputs_tuple))\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\autograd\\functional.py:574\u001b[0m, in \u001b[0;36mjacobian\u001b[1;34m(func, inputs, create_graph, strict, vectorize, strategy)\u001b[0m\n\u001b[0;32m    571\u001b[0m is_inputs_tuple, inputs \u001b[38;5;241m=\u001b[39m _as_tuple(inputs, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minputs\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjacobian\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    572\u001b[0m inputs \u001b[38;5;241m=\u001b[39m _grad_preprocess(inputs, create_graph\u001b[38;5;241m=\u001b[39mcreate_graph, need_graph\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m--> 574\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    575\u001b[0m is_outputs_tuple, outputs \u001b[38;5;241m=\u001b[39m _as_tuple(outputs,\n\u001b[0;32m    576\u001b[0m                                       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutputs of the user-provided function\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    577\u001b[0m                                       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjacobian\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    578\u001b[0m _check_requires_grad(outputs, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutputs\u001b[39m\u001b[38;5;124m\"\u001b[39m, strict\u001b[38;5;241m=\u001b[39mstrict)\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\autograd\\functional.py:803\u001b[0m, in \u001b[0;36mhessian.<locals>.jac_func\u001b[1;34m(*inp)\u001b[0m\n\u001b[0;32m    799\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m outer_jacobian_strategy \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mforward-mode\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    800\u001b[0m     \u001b[38;5;66;03m# _grad_preprocess requires create_graph=True and input to require_grad\u001b[39;00m\n\u001b[0;32m    801\u001b[0m     \u001b[38;5;66;03m# or else the input will be detached\u001b[39;00m\n\u001b[0;32m    802\u001b[0m     inp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(t\u001b[38;5;241m.\u001b[39mrequires_grad_(\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m inp)\n\u001b[1;32m--> 803\u001b[0m jac \u001b[38;5;241m=\u001b[39m \u001b[43mjacobian\u001b[49m\u001b[43m(\u001b[49m\u001b[43mensure_single_output_function\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    804\u001b[0m _check_requires_grad(jac, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjacobian\u001b[39m\u001b[38;5;124m\"\u001b[39m, strict\u001b[38;5;241m=\u001b[39mstrict)\n\u001b[0;32m    805\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m jac\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\autograd\\functional.py:574\u001b[0m, in \u001b[0;36mjacobian\u001b[1;34m(func, inputs, create_graph, strict, vectorize, strategy)\u001b[0m\n\u001b[0;32m    571\u001b[0m is_inputs_tuple, inputs \u001b[38;5;241m=\u001b[39m _as_tuple(inputs, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minputs\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjacobian\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    572\u001b[0m inputs \u001b[38;5;241m=\u001b[39m _grad_preprocess(inputs, create_graph\u001b[38;5;241m=\u001b[39mcreate_graph, need_graph\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m--> 574\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    575\u001b[0m is_outputs_tuple, outputs \u001b[38;5;241m=\u001b[39m _as_tuple(outputs,\n\u001b[0;32m    576\u001b[0m                                       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutputs of the user-provided function\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    577\u001b[0m                                       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjacobian\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    578\u001b[0m _check_requires_grad(outputs, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutputs\u001b[39m\u001b[38;5;124m\"\u001b[39m, strict\u001b[38;5;241m=\u001b[39mstrict)\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\autograd\\functional.py:786\u001b[0m, in \u001b[0;36mhessian.<locals>.ensure_single_output_function\u001b[1;34m(*inp)\u001b[0m\n\u001b[0;32m    785\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mensure_single_output_function\u001b[39m(\u001b[38;5;241m*\u001b[39minp):\n\u001b[1;32m--> 786\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    787\u001b[0m     is_out_tuple, t_out \u001b[38;5;241m=\u001b[39m _as_tuple(out, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutputs of the user-provided function\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhessian\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    788\u001b[0m     _check_requires_grad(t_out, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutputs\u001b[39m\u001b[38;5;124m\"\u001b[39m, strict\u001b[38;5;241m=\u001b[39mstrict)\n",
      "\u001b[1;31mTypeError\u001b[0m: pow_adder_reducer() missing 1 required positional argument: 'y'"
     ]
    }
   ],
   "source": [
    "import torch.autograd.functional as F\n",
    "def pow_adder_reducer(x, y):\n",
    "   return (2 * x.pow(2) + 3 * y.pow(2)).sum()\n",
    "inputs = torch.rand(2)\n",
    "F.hessian(pow_adder_reducer, inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eba2266",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
